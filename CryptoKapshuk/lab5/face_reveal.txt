We specify the set of light-field pixels in the following manner. We assume that there are
only a finite set of poses 1, 2, . . . , P in which the face can occur. Each face image is first classified
into the nearest pose. (Although this assumption is clearly an approximation, its validity is
demonstrated by the empirical results in Section 2.3. In both the FERET [39] and PIE [46] databases,
there is considerable variation in the pose of the faces. Although the subjects are asked
to place their face in a fixed pose, they rarely do this perfectly. Both databases therefore contain
considerable variation away from the finite set of poses. Our algorithm performs well on both
databases, so the approximation of classifying faces into a finite set of poses is validated.)
Each pose i = 1, . . . , P is then allocated a fixed number of pixels Ki.  If we have images from poses 3
and 7, for example, we know K3 + K7 of the K pixels in the light-field vector. The remaining
K?K3?K7 are unknown, missing data. This vectorization process is illustrated in Figure 9.3.
We still need to specify how to sample the Ki pixels of a face in pose i. This process is
analogous to that needed in appearance-based object recognition and is usually performed by
“normalization.” In eigenfaces [47], the standard approach is to find the positions of several
canonical points, typically the eyes and the nose, and to warp the input image onto a coordinate
frame where these points are in fixed locations. The resulting image is then masked. To generalize
eigenface normalization to eigen light-fields, we just need to define such a normalization
for each pose.
We report results using two different normalizations. The first is a simple one based on
the location of the eyes and the nose. Just as in eigenfaces, we assume that the eye and nose
locations are known, warp the face into a coordinate frame in which these canonical points
are in a fixed location, and finally crop the image with a (pose-dependent) mask to yield the
Ki pixels. For this simple three-point normalization, the resulting masked images vary in size
between 7200 and 12,600 pixels, depending on the pose.
The second normalization is more complex and is motivated by the success of active appearance
models (AAMs) [12]. This normalization is based on the location of a large number
(39–54 depending on the pose) of points on the face. These canonical points are triangulated
and the image warped with a piecewise affine warp onto a coordinate frame in which the canonical
points are in fixed locations. The resulting masked images for this multipoint normalization
vary in size between 20,800 and 36,000 pixels. Although currently the multipoint normalization
is performed using hand-marked points, it could be performed by fitting an AAM [12] and then
using the implied canonical point locations.